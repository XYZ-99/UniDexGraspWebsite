<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>UniDexGrasp</title>
    <!-- Bootstrap -->
    <link rel="preconnect" href="https://rsms.me/">
    <link rel="stylesheet" href="https://rsms.me/inter/inter.css">
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link href="css/main.css" rel="stylesheet">
    <!-- <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css"> -->
    <style>
      body {
        background: rgb(255, 255, 255) no-repeat fixed top left; 
        font-family: "Inter", 'Open Sans', sans-serif;
      }
    </style>

  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container-fluid">
        <div class="row">
          <div class="col">
            <h2 style="font-size:30px;">UniDexGrasp: Universal Robotic Dexterous Grasping 
              via Learning Diverse Proposal Generation and Goal-Conditioned Policy</h2>
            <h4 style="color:#6e6e6e;"> CVPR 2023 </h4>
            <hr>
            <h6> 
              <a href="https://xyz-99.github.io/">Yinzhen Xu</a><sup>*1, 2, 3</sup>&nbsp; &nbsp;
              <a href="https://wkwan7.github.io/">Weikang Wan</a><sup>*1, 2</sup> &nbsp; &nbsp;
              Jialiang Zhang<sup>*1, 2</sup>&nbsp; &nbsp;
              Haoran Liu<sup>*1, 2</sup>&nbsp; &nbsp;
              Zikang Shan<sup>1</sup> &nbsp; &nbsp;
              Hao Shen<sup>1</sup> &nbsp; &nbsp;
              Ruicheng Wang<sup>1</sup>&nbsp; &nbsp;
              <a href="https://geng-haoran.github.io/">Haoran Geng</a><sup>1, 2</sup>&nbsp; &nbsp;
              <a href="https://yijiaweng.github.io/">Yijia Weng</a><sup>4</sup>&nbsp; &nbsp; 
              <a href="https://jychen18.github.io/">Jiayi Chen</a><sup>1</sup>&nbsp; &nbsp; 
              <a href="https://tengyu.ai/">Tengyu Liu</a><sup>3</sup>&nbsp; &nbsp;
              <a href="https://ericyi.github.io/">Li Yi</a><sup>5</sup>&nbsp; &nbsp; 
              <a href="https://hughw19.github.io/">He Wang</a><sup>†1, 2</sup>
              <br>
              <br>
            <p> 
              <sup>1</sup> Center on Frontiers of Computing Studies, Peking University &nbsp; &nbsp; 
              <sup>2</sup> School of EECS, Peking University&nbsp; &nbsp;  
              <sup>3</sup> Beijing Institute for General AI&nbsp; &nbsp; 
              <sup>4</sup> Stanford University&nbsp; &nbsp; 
              <sup>5</sup> Tsinghua University&nbsp; &nbsp;
              <br>
            </p>
            <p> <sup>*</sup> equal contributions &nbsp;
              <sup>†</sup> corresponding author &nbsp;
              <br>
          </p>
            <!-- <p> <a class="btn btn-secondary btn-lg" href="" role="button">Paper</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Code</a> 
                <a class="btn btn-secondary btn-lg" href="" role="button">Data</a> </p> -->

            <div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5">
                    <a class="btn btn-large btn-light" href="https://arxiv.org/abs/2303.00938" role="button" target="_blank">
                    <i class="fa fa-file"></i> Paper </a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/PKU-EPIC/UniDexGrasp" role="button" target="_blank">
                <i class="fa fa-github-alt"></i> Code </a> </p>
              </div>
              <!-- <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="./index.html" role="button" target="_blank" style="pointer-events: none">
                <i class="fa fa-github-alt"></i> Dataset (Coming soon) </a> </p>
              </div> -->
              <div class="column">
                <p class="mb-5"><a class="btn btn-large btn-light" href="https://mirrors.pku.edu.cn/dl-release/UniDexGrasp_CVPR2023" role="button" target="_blank">
              <i class="fa fa-github-alt"></i> Dataset </a> </p>
            </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <hr style="margin-top:0px">
              <div class="row justify-content-center" style="align-items:center; display:flex;">
               <img src="images/teaser.png" alt="input" class="img-responsive graph" width="95%"/>
              <br>
            </div>
            <p class="text-justify">
              <b> UniDexGrasp via grasp proposal generation and goal-conditioned execution.</b> Left (grasp proposals): each figure shows for
              an object we generate diverse and high-quality grasp poses that vary greatly in rotation, translation and articulation states; right (grasp
              execution): Given two different grasp goal poses illustrated in the two bottom corners, we learn highly generalizable goal-conditioned
              grasping policy that can adaptively execute each corresponding goal pose, respectively shown in the green and blue trajectories.
            </p>
              <!-- </div> -->
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2><strong>Video</strong></h2>
            <hr style="margin-top:0px">
            <div class="row justify-content-center" style="align-items:center; display:flex;">
              <!-- <video width="80%" playsinline="" preload="" muted="" controls>
                <source src="videos/Video4.mp4" type="video/mp4">
              </video> -->
              <iframe width="95%" style="aspect-ratio: 16/9;" src="https://www.youtube.com/embed/HR2JqApZKBs" title="UniDexGrasp Video" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
            </div>  
        </div>
        </div>
      </div>
    </div>
  </section>
  <br>
  <br>

  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2><strong>Abstract</strong></h2>
            <hr style="margin-top:0px">
              <!-- <div class="row justify-content-center" style="align-items:center; display:flex;">
               <img src="images/teaser.png" alt="input" class="img-responsive" width="95%"/>
              <br>
            </div> -->
          <!-- <p class="text-justify">
              <h6 style="color:#8899a5;text-align:left"> Figure 1. Framework overview (From the left to right): we leverage domain randomization-enhanced depth simulation to generate paired data, on which we can train our depth restoration network SwinDRNet, and the restored depths will be fed to downstream tasks and improves estimating category-level pose and grasping for specular and transparent objects.</h6>
          </p> -->
            <p class="text-justify">
              In this work, we tackle the problem of learning universal
              robotic dexterous grasping from a point cloud observation
              under a table-top setting. The goal is to grasp and lift up objects 
              in high-quality and diverse ways and generalize across
              hundreds of categories and even the unseen. 
              <br><br>
              Inspired by
              successful pipelines used in parallel gripper grasping, we
              split the task into two stages: <br>
              <ol>
              <li> grasp proposal (pose) generation;</li>
              <li> goal-conditioned grasp execution.</li>
              </ol>
              <span style="font-weight: 600;"><em>For the first stage</em></span>, we propose a novel probabilistic model of grasp
              pose conditioned on the point cloud observation that factorizes 
              rotation modeling and translation and articulation
              modeling. Trained on our synthesized large-scale dexterous grasp dataset, 
              this model enables us to sample diverse
              and high-quality dexterous grasp poses for the object in the
              point cloud. <span style="font-weight: 600;"><em>For the second stage</em></span>, 
              we propose to replace the motion planning used in parallel gripper grasping with 
              a goal-conditioned grasp policy, due to the complexity involved
              in dexterous grasping execution. Note that it is very challenging
              to learn this highly generalizable grasp policy that only takes 
              realistic inputs without oracle states. 
              <br><br>
              We thus propose several 
              important innovations, including state canonicalization, object 
              curriculum, and teacher-student distillation. When integrating 
              the two stages together, our final pipeline, for the first time,
               shows universal dexterous grasping on thousands of object instances 
               with more than 60% success rate and significantly outperforms all baselines.
              Our experiments show minimal generalization gap between the seen and unseen 
              instances, further demonstrating the universality of our method.
          </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2><strong>Methods</strong></h2>
            <hr style="margin-top:0px">
            <h3 style="margin-top:20px; margin-bottom:20px; color:#717980"><b>Full pipeline</b></h3>
              <div class="row justify-content-center" style="align-items:center; display:flex;">
                <img src="images/MainPipeline.png" alt="input" class="img-responsive graph" width="95%"/>
              </div>
              <p class="text-justify">
                <b>Our main pipeline.</b> The left part is the first stage, 
                which generates a dexterous grasp proposal. 
                The input is the object point cloud at time step 0, 
                $X_0$, fused from depth images, with ground truth segmentation of the table and the object. 
                A rotation $R$ is sampled from the distribution implied by the GraspIPDF, 
                and the point cloud will be canonicalized by $R^{-1}$ to $\tilde{X}_0$. 
                The GraspGlow then samples the translation $\tilde{\bm{t}}$ and joint angles $\bm{q}$. 
                Next, the ContactNet takes $\tilde{X}_0$ and a point cloud $\tilde{X}_H$ 
                sampled from the hand to predict the ideal contact map $\bm{c}$ on the object. 
                Then, the predicted hand pose is optimized based on the contact information. 
                The final goal pose is transformed by $R$ to align with the original visual observation. 
                The right part is the second stage, the goal-conditioned dexterous grasping policy 
                that takes the goal $\bm{g}$, point cloud $X_t$ 
                and robot proprioception $\bm{s}^r_t$ to take actions accordingly.
              </p>
            <br>


            <h3 style="margin-top:20px; margin-bottom:20px; color:#717980"><b>Grasping Policy</b></h3>
            <div class="row justify-content-center" style="align-items:center; display:flex;">
              <img style="padding: 0.5em 0 1em 0;" src="images/policyPipeline.png" alt="input" class="img-responsive graph" width="85%"/>
            </div>
            <p class="text-justify">
              <b>The goal-conditioned dexterous grasping policy pipeline</b>. 
              $\widetilde{{\mathcal{S}}^{\mathcal{E}}}=(\widetilde{\bm{s}_r},\widetilde{\bm{s}_o},X_O,\widetilde{g})$ 
              and $\widetilde{{\mathcal{S}}^{\mathcal{S}}}=(\widetilde{\bm{s}_r},\widetilde{X_S},\widetilde{g})$ denote 
              the input state of the teacher policy and student policy after state canonicalization, respectively;
              $\oplus$ denotes concatenation.
            </p>
            <br>
            <!-- <h3 style="margin-top:20px; margin-bottom:20px; color:#717980"><b>Robotic Grasping</b></h3> -->
            <!-- <div class="row justify-content-center" style="align-items:center; display:flex;">
              <video width="80%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" controls>
                <source src="videos/grasping.mp4" type="video/mp4">
              </video>
            </div>   -->
        </div>
        </div>
      </div>
    </div>
  </section>
  <br>
  <br>

  
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2><strong>Language-guided Dexterous Grasping</strong></h2>
            <hr style="margin-top:0px">
              <div class="row justify-content-center" style="align-items:center; display:flex;">
                <img src="images/language.png" alt="input" class="img-responsive graph" width="60%"/>
              </div>
              <p class="text-justify">
                <b>Qualitative results of language-guided grasp proposal selection</b>.
                CLIP can select proposals complying with the language instruction, 
                allowing goal-conditioned policy to execute potentially functional grasps.
              </p>
            <br>
        </div>
        </div>
      </div>
    </div>
  </section>
  <br>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2><strong>Qualitative results</strong></h2>
          <hr style="margin-top:0px">
          <div class="row justify-content-center" style="align-items:center; display:flex;">
            <img src="images/gallery2.png" alt="input" class="img-responsive graph" width="60%"/>
          </div>
          <div class="row justify-content-center" style="align-items:center; display:flex;">
            <img src="images/gallery.png" alt="input" class="img-responsive graph" width="100%"/>
          </div>
        </div>
        </div>
      </div>
    </div>
  </section>
  <br>
  <br>

  <!-- citing -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h2><strong>Citation</strong></h2>
          <hr style="margin-top:0px">
              <!-- <pre style="background-color: #e9eeef;padding: 1.25em 1.5em"> -->
<pre style="background-color: #e9eeef;padding: 0 1.5em">
<code>
@article{xu2023unidexgrasp,
  title={UniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal Generation and Goal-Conditioned Policy},
  author={Xu, Yinzhen and Wan, Weikang and Zhang, Jialiang and Liu, Haoran and Shan, Zikang and Shen, Hao and Wang, Ruicheng and Geng, Haoran and Weng, Yijia and Chen, Jiayi and others},
  journal={arXiv preprint arXiv:2303.00938},
  year={2023}
}
</code>
</pre>
      </div>
    </div>
  </div>
  <br>

    <!-- Contact -->
    <div class="container">
      <div class="row ">
        <div class="col-12">
            <h2><strong>Contact</strong></h2>
            <hr style="margin-top:0px">
            <p>If you have any questions, please feel free to contact us:
              <ul>
                <li><b>Yinzhen Xu</b>&colon; xuyinzhen.hi<span style="display:none">Prevent spamming</span>@<span style="display:none">Prevent spamming</span>gmail.com </li>
                <li><b>Weikang Wan</b>&colon; wwk<span style="display:none">Prevent spamming</span>@<span style="display:none">Prevent spamming</span>pku.edu.cn </li>
                <li><b>Jialiang Zhang</b>&colon; jackzhang0906<span style="display:none">Prevent spamming</span>@<span style="display:none">Prevent spamming</span>126.com </li>
                <li><b>Haoran Liu</b>&colon; lhrrhl0419<span style="display:none">Prevent spamming</span>@<span style="display:none">Prevent spamming</span>163.com </li>
                <li><b>He Wang</b>&colon; hewang<span style="display:none">Prevent spamming</span>@<span style="display:none">Prevent spamming</span>pku.edu.cn </li>
              </ul>
            </p>
        </pre>
        </div>
      </div>
    </div>



  <footer class="text-center" style="margin-bottom:10px; font-size: medium;">
      <hr>
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the <a href="https://lioryariv.github.io/idr/" target="_blank">website template</a>.
  </footer>
  <script>
    MathJax = {
      tex: {inlineMath: [['$', '$'], ['\\(', '\\)']],
      macros: {
        bm: ["{\\boldsymbol #1}",1],
      }}
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</body>
</html>
